{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Table of Contents                               \n",
    "{:toc}      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "**HDFS**  \n",
    "Open dataset인 [Hadoop Distributed File System(HDFS)](https://github.com/logpai/loghub/blob/master/HDFS/HDFS_2k.log) 데이터셋은 하둡 분산처리 시스템의 로그로 이루어져 있습니다.  \n",
    "HDFS 데이터셋은 정상 시스템 로그와 이상 시스템 로그로 이루어져 있습니다.  \n",
    "로그 내의 **block_id**는 해당 로그가 발생한 사건의 고유 정보를 의미합니다.     \n",
    "같은 **block_id** 데이터셋은 모두 같은 사건에 대한 정보를 담고 있습니다.  \n",
    "따라서 **block_id** 데이터를 묶어 같은 **block_id** 이루어진 로그 시퀀스는 한 개의 로그 시퀀스로 판단합니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log Parser**  \n",
    "Log Parser는 비정형 로그 데이터를 정형화된 템플릿으로 변환하는 역할을 수행합니다.  \n",
    "본 데이터셋에 대해서는 [Drain Log Parser](https://github.com/logpai/logparser/tree/master/logparser/Drain)를 이용합니다.  \n",
    "데이터에 대해 Parser를 학습하면 정형화된 템플릿 규칙을 만들어냅니다.   \n",
    "예시를 들어보겠습니다. 아래는 HDFS 데이터셋에 존재하는 기존 로그 파일입니다.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "081109 204015 308 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_8229193803249955061 terminating\n",
    "081109 204106 329 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-6670958622368987959 terminating\n",
    "081109 204132 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.43.115:50010 is added to blk_3050920587428079149 size 67108864\n",
    "081109 204324 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.203.80:50010 is added to blk_7888946331804732825 size 67108864\n",
    "081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864\n",
    "081109 204525 512 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_572492839287299681 terminating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 Drain Log Parser를 HDFS 데이터셋에 대해서 학습을 시켜 추출한 템플릿입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>Occurrences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09a53393</td>\n",
       "      <td>Receiving block &lt;*&gt; src: &lt;*&gt; dest: &lt;*&gt;</td>\n",
       "      <td>1341924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3d91fa85</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: &lt;*&gt; &lt;*&gt;</td>\n",
       "      <td>446578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dc2c74b7</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block &lt;*&gt; terminating</td>\n",
       "      <td>1339734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e3df2680</td>\n",
       "      <td>Received block &lt;*&gt; of size &lt;*&gt; from &lt;*&gt;</td>\n",
       "      <td>1339734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d5de21c</td>\n",
       "      <td>BLOCK* NameSystem.addStoredBlock: blockMap upd...</td>\n",
       "      <td>1343739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    EventId                                      EventTemplate  Occurrences\n",
       "0  09a53393             Receiving block <*> src: <*> dest: <*>      1341924\n",
       "1  3d91fa85           BLOCK* NameSystem.allocateBlock: <*> <*>       446578\n",
       "2  dc2c74b7      PacketResponder <*> for block <*> terminating      1339734\n",
       "3  e3df2680            Received block <*> of size <*> from <*>      1339734\n",
       "4  5d5de21c  BLOCK* NameSystem.addStoredBlock: blockMap upd...      1343739"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates = pd.read_csv('./data/train_templates.csv')\n",
    "templates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 템플릿에 따르면 위의 데이터는  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2  \n",
    "2  \n",
    "5  \n",
    "5  \n",
    "5  \n",
    "2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로 변하게 됩니다.\n",
    "\n",
    "본 튜토리얼에서는 Log Parser에 대한 자세한 학습 과정 및 설명은 생략합니다.  \n",
    "data 폴더 내의 train.pkl과 test.pkl은 학습된 parser를 통해 전처리가 진행된 파일입니다.  \n",
    "\n",
    "{block_id : log sequence} 형식의 dictionary입니다. 예시는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('./data/test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blk_-1000195927844309648',\n",
       "  [0, 0, 0, 3, 17, 18, 17, 18, 17, 18, 6, 6, 6, 14, 14, 14, 16, 16, 16]),\n",
       " ('blk_-1000245396392748444',\n",
       "  [3, 0, 0, 0, 6, 6, 6, 17, 18, 17, 18, 17, 18, 2, 14, 14, 14, 16, 16, 16])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.items())[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Encoding\n",
    "데이터를 모델에 넣기 위해서는 인코딩 과정이 필요합니다.  \n",
    "본 과정에서는 이를 위해 간단한 Vocab 모듈을 제작해 사용합니다.  \n",
    "Vocab 모듈은 input 데이터의 각 원소들을 해당 원소에 해당하는 embedding matrix에 매칭시키는 역할을 수행합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    ''' 데이터 인코딩에 필요한 vocab을 만드는 모듈 '''\n",
    "    def __init__(self, vocab_path):\n",
    "        # 모델에 필요한 special token을 정의합니다.\n",
    "        self.special_tokens = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
    "        self.vocab_path = vocab_path\n",
    "        self.data = None\n",
    "        self.vocab = None\n",
    "        if os.path.isfile(vocab_path):\n",
    "            self.load_vocab()\n",
    "\n",
    "    def load_data(self, data_path):\n",
    "        with open(data_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.data = list(data.values())\n",
    "\n",
    "    def create_vocab(self):\n",
    "        assert self.data is not None\n",
    "        ctr = Counter()\n",
    "        for d in self.data:\n",
    "            ctr.update(d)\n",
    "        vocab = list(ctr.keys())\n",
    "        self.vocab = self.special_tokens + vocab\n",
    "        self.save_vocab()\n",
    "\n",
    "    def load_vocab(self):\n",
    "        with open(self.vocab_path, 'r') as f:\n",
    "            vocab = []\n",
    "            for line in f:\n",
    "                try:\n",
    "                    vocab.append(int(line.strip()))\n",
    "                except:\n",
    "                    vocab.append(line.strip())\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def save_vocab(self):\n",
    "        assert self.vocab is not None\n",
    "        with open(self.vocab_path, 'w') as f:\n",
    "            for word in self.vocab:\n",
    "                f.write(str(word) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/train.pkl'  # vocab 파일을 제작할 파일 (train set)\n",
    "vocab_path = './data/vocab.txt'  # 만들어진 vocab 파일의 위치  \n",
    "\n",
    "# Create vocab.txt\n",
    "vocab_module = Vocab(vocab_path)\n",
    "vocab_module.load_data(data_path)\n",
    "vocab_module.create_vocab()\n",
    "log_vocab = vocab_module.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<BOS>',\n",
       " '<EOS>',\n",
       " '<UNK>',\n",
       " 0,\n",
       " 3,\n",
       " 17,\n",
       " 18,\n",
       " 6,\n",
       " 11,\n",
       " 7,\n",
       " 14,\n",
       " 16,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 15,\n",
       " 5,\n",
       " 10,\n",
       " 13,\n",
       " 1,\n",
       " 12]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Custom Datasets\n",
    "파이토치는 직접 datasets 모듈을 만들어 사용할 수 있습니다.  \n",
    "이 때 필요한 것은 해당 모듈에  \n",
    "\\__len__  \n",
    "\\__getitem__  \n",
    "메소드를 오버라이딩해어야 한다는 것입니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLoader(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.blk_id, self.data, self.lengths = self.data_load()\n",
    "        # Special tokens 정의\n",
    "        self.pad, self.bos, self.eos, self.unk = 0, 1, 2, 3\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        input_log, output_log = self.preprocess(self.data[item])\n",
    "        return input_log, output_log, self.lengths[item]\n",
    "\n",
    "    def data_load(self):\n",
    "        with open(self.data_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        blk, logs = list(zip(*data.items()))\n",
    "        lengths = [len(d) + 1 for d in logs]\n",
    "        return blk, logs, lengths\n",
    "\n",
    "    def preprocess(self, log):\n",
    "        # Special token 추가를 위해 input 조정\n",
    "        log = [int(l) + 4 for l in log]\n",
    "        # eos, bos 추가\n",
    "        logs = (log + [self.eos], [self.bos] + log)\n",
    "        return logs\n",
    "\n",
    "    def padding(self, log, max_len):\n",
    "        return [l + [self.pad] * (max_len - len(l)) for l in log]\n",
    "\n",
    "    def batch_sequence(self, batch):\n",
    "        input_log, output_log, lengths = list(zip(*batch))\n",
    "        assert len(input_log) == len(output_log)\n",
    "        assert len(input_log) == len(lengths)\n",
    "\n",
    "        max_seq_length = max(lengths)\n",
    "\n",
    "        # pad tokens\n",
    "        input_log, output_log = [self.padding(log, max_seq_length) for log in (input_log, output_log)]\n",
    "        assert len(output_log[0]) == max_seq_length\n",
    "\n",
    "        return (torch.tensor(d) for d in (input_log, output_log, lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('blk_-1000002529962039464', [0, 0, 0, 3, 17, 18, 17, 18, 6, 6, 6, 17, 18])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(log.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventId</th>\n",
       "      <th>EventTemplate</th>\n",
       "      <th>Occurrences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09a53393</td>\n",
       "      <td>Receiving block &lt;*&gt; src: &lt;*&gt; dest: &lt;*&gt;</td>\n",
       "      <td>381308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3d91fa85</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: &lt;*&gt; &lt;*&gt;</td>\n",
       "      <td>128483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dc2c74b7</td>\n",
       "      <td>PacketResponder &lt;*&gt; for block &lt;*&gt; terminating</td>\n",
       "      <td>366945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e3df2680</td>\n",
       "      <td>Received block &lt;*&gt; of size &lt;*&gt; from &lt;*&gt;</td>\n",
       "      <td>366780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5d5de21c</td>\n",
       "      <td>BLOCK* NameSystem.addStoredBlock: blockMap upd...</td>\n",
       "      <td>376002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    EventId                                      EventTemplate  Occurrences\n",
       "0  09a53393             Receiving block <*> src: <*> dest: <*>       381308\n",
       "1  3d91fa85           BLOCK* NameSystem.allocateBlock: <*> <*>       128483\n",
       "2  dc2c74b7      PacketResponder <*> for block <*> terminating       366945\n",
       "3  e3df2680            Received block <*> of size <*> from <*>       366780\n",
       "4  5d5de21c  BLOCK* NameSystem.addStoredBlock: blockMap upd...       376002"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Pytorch Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = list(zip(*log.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 3, 17, 18, 17, 18, 6, 6, 6, 17, 18]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-5520c23f2f1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;34m'test'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "'test'.astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-senti",
   "language": "python",
   "name": "torch-senti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
